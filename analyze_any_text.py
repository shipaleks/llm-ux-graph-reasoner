#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ (–ª—é–±–æ–≥–æ —è–∑—ã–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞).
–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    ./analyze_any_text.py <–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É> [--output <–≤—ã—Ö–æ–¥–Ω–∞—è_–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è>]
"""

import os
import sys
import json
import logging
import argparse
import time
import re
import asyncio
from pathlib import Path
from datetime import datetime
import uuid
from typing import List, Dict, Any, Optional, Tuple

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

try:
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏ –∏–∑ —Å–∏—Å—Ç–µ–º—ã
    from src.knowledge_graph_synth.models.segment import TextSegment, SegmentCollection
    from src.knowledge_graph_synth.llm.gemini_reasoning import GeminiReasoningProvider
    from src.knowledge_graph_synth.text.loader import TextLoader
except ImportError:
    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏ –∏–∑ —Å–∏—Å—Ç–µ–º—ã.")
    logger.error("–ó–∞–ø—É—Å–∫–∞—é –∞–≤—Ç–æ–Ω–æ–º–Ω—É—é –≤–µ—Ä—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã
    class TextSegment:
        """–¢–µ–∫—Å—Ç–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç."""
        
        def __init__(self, 
                   document_id: str,
                   text: str,
                   language: str = "en",
                   id = None,
                   parent_id = None,
                   start_position = None,
                   end_position = None,
                   metadata = None):
            """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞."""
            self.id = id or uuid.uuid4()
            self.document_id = document_id
            self.text = text
            self.language = language
            self.parent_id = parent_id
            self.start_position = start_position
            self.end_position = end_position
            self.metadata = metadata or {}
            self.length = len(text) if text else 0
        
        def to_dict(self):
            """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å."""
            return {
                "id": str(self.id),
                "document_id": self.document_id,
                "text": self.text,
                "language": self.language,
                "parent_id": str(self.parent_id) if self.parent_id else None,
                "start_position": self.start_position,
                "end_position": self.end_position,
                "metadata": self.metadata
            }
    
    class SegmentCollection:
        """–ö–æ–ª–ª–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤."""
        
        def __init__(self):
            """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
            self.segments = {}
        
        def add_segment(self, segment):
            """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é."""
            self.segments[str(segment.id)] = segment
            return segment
        
        def get_root_segments(self):
            """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤."""
            return [s for s in self.segments.values() if s.parent_id is None]
    
    class TextLoader:
        """–ó–∞–≥—Ä—É–∑—á–∏–∫ —Ç–µ–∫—Å—Ç–∞."""
        
        def load_file(self, file_path, document_id=None):
            """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞."""
            if not document_id:
                document_id = f"doc_{int(time.time())}"
                
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
            language = self._detect_language(text)
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            return TextDocument(document_id, text, language)
        
        def _detect_language(self, text):
            """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞."""
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            cyrillic_count = sum(1 for c in text if '–∞' <= c.lower() <= '—è')
            total_chars = sum(1 for c in text if c.isalpha())
            
            if total_chars == 0:
                return "en"
                
            cyrillic_ratio = cyrillic_count / total_chars
            return "ru" if cyrillic_ratio > 0.3 else "en"
    
    class TextDocument:
        """–¢–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç."""
        
        def __init__(self, id, text, language):
            """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
            self.id = id
            self.text = text
            self.language = language
    
    class GeminiReasoningProvider:
        """–ü—Ä–æ–≤–∞–π–¥–µ—Ä Gemini –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."""
        
        def __init__(self, config=None):
            """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
            self.config = config or {}
            self.last_call = 0
            
        async def generate_text(self, prompt, model=None, **kwargs):
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
            logger.info("ü§ñ –ó–∞–ø—Ä–æ—Å –∫ LLM API...")
            
            # –ò–º–∏—Ç–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏ —Å–µ—Ç–∏
            await asyncio.sleep(1)
            
            # –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è –∏–º–∏—Ç–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã –∑–∞–ø—Ä–æ—Å –∫ API)
            response = f"–û—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å: {prompt[:50]}..."
            
            return response
            
        async def generate_structured(self, prompt, response_schema, model=None, **kwargs):
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
            logger.info("ü§ñ –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ LLM API...")
            
            # –ò–º–∏—Ç–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏ —Å–µ—Ç–∏
            await asyncio.sleep(1)
            
            # –ü—Ä–æ—Å—Ç–æ–π —à–∞–±–ª–æ–Ω –æ—Ç–≤–µ—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ö–µ–º—ã
            if "segments" in response_schema.get("properties", {}):
                # –°—Ö–µ–º–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
                return {
                    "segments": [
                        {"number": 1, "title": "–ü–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç", "full_text": "–¢–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"},
                        {"number": 2, "title": "–í—Ç–æ—Ä–æ–π —Å–µ–≥–º–µ–Ω—Ç", "full_text": "–¢–µ–∫—Å—Ç –≤—Ç–æ—Ä–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"}
                    ]
                }
            elif response_schema.get("type") == "array":
                # –°—Ö–µ–º–∞ —Ç–µ–æ—Ä–∏–π
                return [
                    {
                        "name": "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–æ—Ä–∏—è",
                        "description": "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–æ—Ä–∏–∏",
                        "confidence": 0.85,
                        "hypotheses": [
                            {
                                "statement": "–ü–µ—Ä–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞",
                                "confidence": 0.8,
                                "evidence": [
                                    {"description": "–ü–µ—Ä–≤–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ", "strength": 0.75}
                                ]
                            }
                        ]
                    }
                ]
            else:
                # –°—Ö–µ–º–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
                return {
                    "title": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–∞",
                    "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞.",
                    "key_points": ["–ü–µ—Ä–≤—ã–π –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç", "–í—Ç–æ—Ä–æ–π –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç"]
                }

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
class UnifiedAnalyzer:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞."""
    
    def __init__(self, output_dir="output/unified_analysis"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞.
        
        Args:
            output_dir: –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        self.output_base = output_dir
        self.timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.output_dir = os.path.join(output_dir, self.timestamp)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LLM
        self.llm = GeminiReasoningProvider()
        
    async def analyze_text(self, file_path: str, max_segments: Optional[int] = None) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É
            max_segments: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –æ—Ç—á–µ—Ç—É
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
        loader = TextLoader()
        document_id = f"doc_{self.timestamp}"
        document = loader.load_file(file_path, document_id=document_id)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        context_dir = os.path.join(self.output_dir, "context")
        theories_dir = os.path.join(self.output_dir, "theories")
        
        os.makedirs(context_dir, exist_ok=True)
        os.makedirs(theories_dir, exist_ok=True)
        
        # –®–∞–≥ 1: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        logger.info("üîç –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...")
        segments = await self._segment_text(document.text, document_id, document.language)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if max_segments and len(segments) > max_segments:
            logger.info(f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ {max_segments} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–∏–∑ {len(segments)})")
            segments = segments[:max_segments]
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        segments_file = os.path.join(context_dir, "segments.json")
        with open(segments_file, "w", encoding="utf-8") as f:
            segments_data = [s.to_dict() for s in segments]
            json.dump(segments_data, f, ensure_ascii=False, indent=2)
        
        # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
        logger.info(f"üìù –°–æ–∑–¥–∞–Ω–∏–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π –¥–ª—è {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
        summaries = await self._generate_summaries(segments, document.language)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
        summaries_file = os.path.join(context_dir, "segment_summaries.json")
        with open(summaries_file, "w", encoding="utf-8") as f:
            json.dump(summaries, f, ensure_ascii=False, indent=2)
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–æ—Ä–∏–π
        logger.info("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–æ—Ä–∏–π...")
        theories = await self._generate_theories(segments, summaries, document.language)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–æ—Ä–∏–π
        theories_file = os.path.join(theories_dir, "theories.json")
        with open(theories_file, "w", encoding="utf-8") as f:
            json.dump(theories, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ markdown-—Ñ–∞–π–ª–∞ —Ç–µ–æ—Ä–∏–π
        theories_md = self._generate_theories_markdown(theories, document.language)
        theories_md_file = os.path.join(theories_dir, "theories.md")
        with open(theories_md_file, "w", encoding="utf-8") as f:
            f.write(theories_md)
        
        # –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        report_path = self._generate_report(file_path, summaries, theories, document.language)
        
        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        return report_path
    
    async def _segment_text(self, text: str, document_id: str, language: str) -> List[TextSegment]:
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç WEBVTT-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–º
        is_webvtt = text.startswith("WEBVTT")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç
        root_segment = TextSegment(
            document_id=document_id,
            text=text,
            language=language,
            metadata={"segment_type": "document"}
        )
        
        # –î–ª—è WEBVTT –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM-—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é
        if is_webvtt:
            logger.info("üìã –û–±–Ω–∞—Ä—É–∂–µ–Ω WEBVTT-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç, –ø—Ä–∏–º–µ–Ω—è–µ–º LLM-—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é...")
            
            # –û—á–∏—Å—Ç–∫–∞ WEBVTT-—Ä–∞–∑–º–µ—Ç–∫–∏
            clean_text = self._clean_webvtt(text)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
            if language == "ru":
                prompt = f"""
                –†–∞–∑–±–µ–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞ 5-10 –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —á–∞—Å—Ç–µ–π (—Å–µ–≥–º–µ–Ω—Ç–æ–≤), 
                –≥—Ä—É–ø–ø–∏—Ä—É—è —Ä–µ–ø–ª–∏–∫–∏ –ø–æ —Å–º—ã—Å–ª—É –∏ —Ç–µ–º–∞—Ç–∏–∫–µ. –ö–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å 
                —Å–≤—è–∑–∞–Ω–Ω—É—é –º—ã—Å–ª—å –∏–ª–∏ –ø–æ–¥—Ç–µ–º—É.
                
                –í–æ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:
                
                {clean_text[:10000]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                """
            else:
                prompt = f"""
                Divide the following transcript into 5-10 logical segments,
                grouping utterances by meaning and topic. Each segment should 
                represent a related thought or subtopic.
                
                Here's the transcript:
                
                {clean_text[:10000]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                """
            
            # –°—Ö–µ–º–∞ –æ—Ç–≤–µ—Ç–∞
            schema = {
                "type": "object",
                "properties": {
                    "segments": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "number": {"type": "number"},
                                "title": {"type": "string"},
                                "full_text": {"type": "string"}
                            }
                        }
                    }
                }
            }
            
            # –ó–∞–ø—Ä–æ—Å –∫ LLM
            try:
                response = await self.llm.generate_structured(prompt, schema)
                
                # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM
                segments = [root_segment]
                
                if "segments" in response:
                    for segment_data in response["segments"]:
                        segment = TextSegment(
                            document_id=document_id,
                            text=segment_data.get("full_text", ""),
                            language=language,
                            parent_id=root_segment.id,
                            metadata={
                                "segment_type": "llm_transcript_topic",
                                "topic_number": segment_data.get("number", 0),
                                "topic_title": segment_data.get("title", "")
                            }
                        )
                        segments.append(segment)
                
                return segments
            
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ LLM-—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {str(e)}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ—Ä–Ω–µ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
                return [root_segment]
        
        # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –∞–±–∑–∞—Ü–∞–º
        else:
            logger.info("üìÑ –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç, –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –∞–±–∑–∞—Ü–∞–º...")
            paragraphs = text.split("\n\n")
            
            segments = [root_segment]
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():
                    segment = TextSegment(
                        document_id=document_id,
                        text=paragraph.strip(),
                        language=language,
                        parent_id=root_segment.id,
                        metadata={
                            "segment_type": "paragraph",
                            "paragraph_number": i + 1
                        }
                    )
                    segments.append(segment)
            
            return segments
    
    def _clean_webvtt(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ WEBVTT –æ—Ç —Å–ª—É–∂–µ–±–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏.
        
        Args:
            text: –¢–µ–∫—Å—Ç WEBVTT
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        lines = text.split('\n')
        cleaned_lines = []
        
        skip_next = False
        for line in lines:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ WEBVTT
            if line.startswith("WEBVTT"):
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if not line.strip():
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ç–∞–π–º–∫–æ–¥–∞–º–∏
            if "-->" in line and any(c.isdigit() for c in line):
                skip_next = False
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫
            if line.strip().isdigit():
                skip_next = True
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥
            if skip_next:
                skip_next = False
                continue
                
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –≤ –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            cleaned_lines.append(line)
        
        return "\n".join(cleaned_lines)
    
    async def _generate_summaries(self, 
                               segments: List[TextSegment], 
                               language: str) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤.
        
        Args:
            segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
        """
        summaries = {}
        
        for segment in segments:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
            if segment.parent_id is None or len(segment.text) < 50:
                continue
                
            logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–∞ {segment.id} ({len(segment.text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
            if language == "ru":
                prompt = f"""
                –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É:
                
                {segment.text}
                
                –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
                1. –ó–∞–≥–æ–ª–æ–≤–æ–∫, –æ—Ç—Ä–∞–∂–∞—é—â–∏–π –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É –æ—Ç—Ä—ã–≤–∫–∞
                2. –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
                3. –°–ø–∏—Å–æ–∫ –∏–∑ 3-5 –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤
                """
            else:
                prompt = f"""
                Analyze the following text segment and provide a concise summary:
                
                {segment.text}
                
                Your response should include:
                1. A title that reflects the main topic
                2. Brief summary (2-3 sentences)
                3. List of 3-5 key points or facts
                """
            
            # –°—Ö–µ–º–∞ –æ—Ç–≤–µ—Ç–∞
            schema = {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "summary": {"type": "string"},
                    "key_points": {
                        "type": "array",
                        "items": {"type": "string"}
                    }
                }
            }
            
            try:
                response = await self.llm.generate_structured(prompt, schema)
                summaries[str(segment.id)] = response
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {segment.id}: {str(e)}")
                # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
                summaries[str(segment.id)] = {
                    "title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫",
                    "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —ç—Ç–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞.",
                    "key_points": ["–°–∏—Å—Ç–µ–º–∞ –Ω–µ —Å–º–æ–≥–ª–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Å–µ–≥–º–µ–Ω—Ç"]
                }
        
        return summaries
    
    async def _generate_theories(self, 
                              segments: List[TextSegment],
                              summaries: Dict[str, Any],
                              language: str) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–æ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π.
        
        Args:
            segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            summaries: –°–ª–æ–≤–∞—Ä—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–æ—Ä–∏–π
        """
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = ""
        for segment_id, summary in summaries.items():
            context += f"### {summary.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')}\n"
            context += f"{summary.get('summary', '')}\n"
            
            if "key_points" in summary and summary["key_points"]:
                if language == "ru":
                    context += "–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:\n"
                else:
                    context += "Key points:\n"
                    
                for point in summary["key_points"]:
                    context += f"- {point}\n"
                    
            context += "\n"
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        if language == "ru":
            prompt = f"""
            –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 2-4 –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–æ—Ä–∏–∏:

            {context}

            –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–æ—Ä–∏–∏ —É–∫–∞–∂–∏:
            1. –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏
            2. –û–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏
            3. –£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ (–æ—Ç 0.0 –¥–æ 1.0)
            4. 2-4 –≥–∏–ø–æ—Ç–µ–∑—ã, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ —Ç–µ–æ—Ä–∏—é
            
            –î–ª—è –∫–∞–∂–¥–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã —É–∫–∞–∂–∏:
            1. –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            2. –£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ (–æ—Ç 0.0 –¥–æ 1.0)
            3. 1-3 –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –≥–∏–ø–æ—Ç–µ–∑—É
            """
        else:
            prompt = f"""
            Based on the following context, formulate 2-4 main theories:

            {context}

            For each theory, provide:
            1. Theory name
            2. Theory description
            3. Confidence level (from 0.0 to 1.0)
            4. 2-4 hypotheses supporting the theory
            
            For each hypothesis, provide:
            1. Statement
            2. Confidence level (from 0.0 to 1.0)
            3. 1-3 pieces of evidence supporting the hypothesis
            """
        
        # –°—Ö–µ–º–∞ –æ—Ç–≤–µ—Ç–∞
        schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "description": {"type": "string"},
                    "confidence": {"type": "number"},
                    "hypotheses": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "statement": {"type": "string"},
                                "confidence": {"type": "number"},
                                "evidence": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "description": {"type": "string"},
                                            "strength": {"type": "number"}
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        
        try:
            theories = await self.llm.generate_structured(prompt, schema)
            logger.info(f"üß† –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(theories)} —Ç–µ–æ—Ä–∏–π")
            return theories
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–æ—Ä–∏–π: {str(e)}")
            # –†–µ–∑–µ—Ä–≤–Ω–∞—è —Ç–µ–æ—Ä–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if language == "ru":
                return [{
                    "name": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ–æ—Ä–∏—è",
                    "description": "–≠—Ç–∞ —Ç–µ–æ—Ä–∏—è –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç.",
                    "confidence": 0.5,
                    "hypotheses": [
                        {
                            "statement": "–í —Ç–µ–∫—Å—Ç–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –∑–Ω–∞—á–∏–º–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
                            "confidence": 0.6,
                            "evidence": [
                                {
                                    "description": "–¢–µ–∫—Å—Ç –±—ã–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                                    "strength": 0.5
                                }
                            ]
                        }
                    ]
                }]
            else:
                return [{
                    "name": "Automatically generated theory",
                    "description": "This theory was created as a fallback option.",
                    "confidence": 0.5,
                    "hypotheses": [
                        {
                            "statement": "The text contains significant information",
                            "confidence": 0.6,
                            "evidence": [
                                {
                                    "description": "The text was long enough for analysis",
                                    "strength": 0.5
                                }
                            ]
                        }
                    ]
                }]
    
    def _generate_theories_markdown(self, theories: List[Dict[str, Any]], language: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Ç–µ–æ—Ä–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.
        
        Args:
            theories: –°–ø–∏—Å–æ–∫ —Ç–µ–æ—Ä–∏–π
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            Markdown-—Ç–µ–∫—Å—Ç
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        labels = {
            "ru": {
                "title": "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–æ—Ä–∏–∏",
                "theory": "–¢–µ–æ—Ä–∏—è",
                "confidence": "–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å",
                "description": "–û–ø–∏—Å–∞–Ω–∏–µ",
                "hypotheses": "–ì–∏–ø–æ—Ç–µ–∑—ã",
                "hypothesis": "–ì–∏–ø–æ—Ç–µ–∑–∞",
                "evidence": "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞",
                "strength": "–°–∏–ª–∞"
            },
            "en": {
                "title": "Generated Theories",
                "theory": "Theory",
                "confidence": "Confidence",
                "description": "Description",
                "hypotheses": "Hypotheses",
                "hypothesis": "Hypothesis",
                "evidence": "Evidence",
                "strength": "Strength"
            }
        }
        
        l = labels.get(language, labels["en"])
        
        md = f"# {l['title']}\n\n"
        
        for i, theory in enumerate(theories):
            md += f"## {l['theory']} {i+1}: {theory.get('name', '')}\n\n"
            md += f"**{l['confidence']}**: {theory.get('confidence', 0.0):.2f}\n\n"
            md += f"**{l['description']}**: {theory.get('description', '')}\n\n"
            
            if "hypotheses" in theory and theory["hypotheses"]:
                md += f"### {l['hypotheses']}\n\n"
                
                for j, hypothesis in enumerate(theory["hypotheses"]):
                    md += f"#### {l['hypothesis']} {j+1}: {hypothesis.get('statement', '')}\n\n"
                    md += f"**{l['confidence']}**: {hypothesis.get('confidence', 0.0):.2f}\n\n"
                    
                    if "evidence" in hypothesis and hypothesis["evidence"]:
                        md += f"**{l['evidence']}**:\n\n"
                        
                        for evidence in hypothesis["evidence"]:
                            md += f"- {evidence.get('description', '')} ({l['strength']}: {evidence.get('strength', 0.0):.2f})\n"
                    
                    md += "\n"
            
            md += "---\n\n"
        
        return md
    
    def _generate_report(self, 
                      file_path: str,
                      summaries: Dict[str, Any],
                      theories: List[Dict[str, Any]],
                      language: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML-–æ—Ç—á–µ—Ç–∞.
        
        Args:
            file_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            summaries: –°–ª–æ–≤–∞—Ä—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
            theories: –°–ø–∏—Å–æ–∫ —Ç–µ–æ—Ä–∏–π
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –æ—Ç—á–µ—Ç—É
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        labels = {
            "ru": {
                "title": "–û—Ç—á–µ—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞",
                "file": "–§–∞–π–ª",
                "date": "–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞",
                "summary": "–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
                "no_title": "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞",
                "key_points": "–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã",
                "theories": "–¢–µ–æ—Ä–∏–∏ –∏ –≥–∏–ø–æ—Ç–µ–∑—ã",
                "no_theories": "–¢–µ–æ—Ä–∏–∏ –Ω–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.",
                "theory": "–¢–µ–æ—Ä–∏—è",
                "hypothesis": "–ì–∏–ø–æ—Ç–µ–∑–∞",
                "confidence": "–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å",
                "evidence": "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞",
                "strength": "–°–∏–ª–∞"
            },
            "en": {
                "title": "Text Analysis Report",
                "file": "File",
                "date": "Analysis Date",
                "summary": "Text Summary",
                "no_title": "No Title",
                "key_points": "Key Points",
                "theories": "Theories and Hypotheses",
                "no_theories": "No theories were generated.",
                "theory": "Theory",
                "hypothesis": "Hypothesis",
                "confidence": "Confidence",
                "evidence": "Evidence",
                "strength": "Strength"
            }
        }
        
        l = labels.get(language, labels["en"])
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        html_content = f"""
        <!DOCTYPE html>
        <html lang="{language}">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{l["title"]}</title>
            <style>
                body {{
                    font-family: Arial, sans-serif;
                    line-height: 1.6;
                    color: #333;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                }}
                h1, h2, h3, h4 {{
                    color: #2c3e50;
                }}
                .summary-container {{
                    background-color: #f5f9ff;
                    border-left: 4px solid #4a90e2;
                    margin-bottom: 30px;
                    padding: 15px;
                    border-radius: 4px;
                }}
                .metadata {{
                    background-color: #f9f9f9;
                    padding: 15px;
                    border-left: 4px solid #2c3e50;
                    margin-bottom: 20px;
                }}
                .key-points {{
                    background-color: #f8f8f8;
                    padding: 15px;
                    border-radius: 4px;
                    margin-top: 10px;
                }}
                .theory {{
                    background-color: #f5f9ff;
                    border-left: 4px solid #4a90e2;
                    margin-bottom: 30px;
                    padding: 15px;
                    border-radius: 4px;
                }}
                .hypothesis {{
                    background-color: #f9f9f9;
                    padding: 15px;
                    margin: 10px 0;
                    border-left: 3px solid #666;
                    border-radius: 4px;
                }}
            </style>
        </head>
        <body>
            <h1>{l["title"]}</h1>
            
            <div class="metadata">
                <p><strong>{l["file"]}:</strong> {os.path.basename(file_path)}</p>
                <p><strong>{l["date"]}:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <h2>{l["summary"]}</h2>
        """
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
        for segment_id, summary in summaries.items():
            title = summary.get("title", l["no_title"])
            summary_text = summary.get("summary", "")
            key_points = summary.get("key_points", [])
            
            key_points_html = ""
            if key_points:
                key_points_html = f"<div class='key-points'><h4>{l['key_points']}:</h4><ul>"
                for point in key_points:
                    key_points_html += f"<li>{point}</li>"
                key_points_html += "</ul></div>"
            
            html_content += f"""
            <div class="summary-container">
                <h3>{title}</h3>
                <p>{summary_text}</p>
                {key_points_html}
            </div>
            """
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–æ—Ä–∏–π
        html_content += f"<h2>{l['theories']}</h2>"
        
        if theories:
            for i, theory in enumerate(theories):
                theory_name = theory.get("name", f"{l['theory']} {i+1}")
                theory_desc = theory.get("description", "")
                theory_conf = theory.get("confidence", 0.0)
                
                hypotheses_html = ""
                if "hypotheses" in theory and theory["hypotheses"]:
                    for h, hypothesis in enumerate(theory["hypotheses"]):
                        h_statement = hypothesis.get("statement", "")
                        h_confidence = hypothesis.get("confidence", 0.0)
                        
                        evidence_html = ""
                        if "evidence" in hypothesis and hypothesis["evidence"]:
                            evidence_html = f"<p><strong>{l['evidence']}:</strong></p><ul>"
                            for ev in hypothesis["evidence"]:
                                ev_desc = ev.get("description", "")
                                ev_strength = ev.get("strength", 0.0)
                                evidence_html += f"<li>{ev_desc} ({l['strength']}: {ev_strength:.2f})</li>"
                            evidence_html += "</ul>"
                        
                        hypotheses_html += f"""
                        <div class="hypothesis">
                            <h4>{l['hypothesis']} {h+1}: {h_statement}</h4>
                            <div>{l['confidence']}: {h_confidence:.2f}</div>
                            {evidence_html}
                        </div>
                        """
                
                html_content += f"""
                <div class="theory">
                    <h3>{theory_name}</h3>
                    <div>{l['confidence']}: {theory_conf:.2f}</div>
                    <p>{theory_desc}</p>
                    {hypotheses_html}
                </div>
                """
        else:
            html_content += f"<p>{l['no_theories']}</p>"
        
        html_content += """
        </body>
        </html>
        """
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = os.path.join(self.output_dir, "report.html")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return report_path

def parse_arguments():
    """–†–∞–∑–±–æ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description="–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤ (–ª—é–±–æ–≥–æ —è–∑—ã–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞)"
    )
    
    parser.add_argument(
        "file_path",
        help="–ü—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
    )
    
    parser.add_argument(
        "--output", "-o",
        default="output/unified_analysis",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: output/unified_analysis)"
    )
    
    parser.add_argument(
        "--max-segments", "-m",
        type=int,
        default=None,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"
    )
    
    return parser.parse_args()

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    args = parse_arguments()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞
    if not os.path.exists(args.file_path):
        logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.file_path}")
        return 1
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = UnifiedAnalyzer(output_dir=args.output)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    start_time = time.time()
    logger.info(f"–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞: {args.file_path}")
    
    try:
        report_path = await analyzer.analyze_text(args.file_path, args.max_segments)
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_path}")
        print("üåê –í—ã –º–æ–∂–µ—Ç–µ –æ—Ç–∫—Ä—ã—Ç—å –µ–≥–æ –≤ –±—Ä–∞—É–∑–µ—Ä–µ.")
        return 0
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        return 1

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    sys.exit(asyncio.run(main()))